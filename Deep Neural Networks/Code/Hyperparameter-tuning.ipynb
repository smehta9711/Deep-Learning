{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981788b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Import the 'optimize' module from SciPy\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591024a5",
   "metadata": {},
   "source": [
    "### Unpack Function ###\n",
    "Function to unpack a single vector of weights into weight matrices and bias vectors for a multi-layer neural network.\n",
    "    \n",
    "Arguments:\n",
    "- weights: Flattened array containing all weights and biases.\n",
    "- NUM_HIDDEN_LAYERS: Number of hidden layers in the neural network.\n",
    "- NUM_INPUT: Number of input features.\n",
    "- NUM_HIDDEN: List specifying the number of units in each hidden layer.\n",
    "- NUM_OUTPUT: Number of output units.\n",
    "\n",
    "Returns:\n",
    "- Ws: List of weight matrices for each layer.\n",
    "- bs: List of bias vectors for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c97fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(weights, NUM_HIDDEN_LAYERS, NUM_INPUT, NUM_HIDDEN, NUM_OUTPUT):\n",
    "    \n",
    "    # Initialize list to store weight matrices\n",
    "    Ws = []\n",
    "\n",
    "    # Unpack the weight matrix for the input layer to the first hidden layer\n",
    "    start = 0\n",
    "    end = NUM_INPUT * NUM_HIDDEN[0]  # Calculate the range for the first weight matrix\n",
    "    W = weights[start:end]  # Slice the weight vector for the first layer's weights\n",
    "    Ws.append(W)  # Append the weight vector to the list\n",
    "\n",
    "    # Unpack weight matrices between hidden layers\n",
    "    for i in range(NUM_HIDDEN_LAYERS - 1):  # Loop through hidden layers\n",
    "        start = end  # Update the start index\n",
    "        end = end + NUM_HIDDEN[i] * NUM_HIDDEN[i+1]  # Calculate the end index for the next layer\n",
    "        W = weights[start:end]  # Slice the weight vector for the current layer's weights\n",
    "        Ws.append(W)  # Append the weight vector to the list\n",
    "\n",
    "    # Unpack the weight matrix for the last hidden layer to the output layer\n",
    "    start = end  # Update the start index for the last layer\n",
    "    end = end + NUM_HIDDEN[-1] * NUM_OUTPUT  # Calculate the end index for the output layer\n",
    "    W = weights[start:end]  # Slice the weight vector for the output layer's weights\n",
    "    Ws.append(W)  # Append the weight vector to the list\n",
    "\n",
    "    # Reshape the weight vectors into matrices for each layer\n",
    "    Ws[0] = Ws[0].reshape(NUM_HIDDEN[0], NUM_INPUT)  # Reshape the first layer (input to hidden)\n",
    "    for i in range(1, NUM_HIDDEN_LAYERS):  # Loop through hidden layers\n",
    "        Ws[i] = Ws[i].reshape(NUM_HIDDEN[i], NUM_HIDDEN[i-1])  # Reshape the hidden-to-hidden weights\n",
    "    Ws[-1] = Ws[-1].reshape(NUM_OUTPUT, NUM_HIDDEN[-1])  # Reshape the final layer (hidden to output)\n",
    "\n",
    "    # Initialize list to store bias vectors\n",
    "    bs = []\n",
    "\n",
    "    # Unpack bias vector for the first hidden layer\n",
    "    start = end  # Set start index for biases\n",
    "    end = end + NUM_HIDDEN[0]  # Calculate the end index for the first bias vector\n",
    "    b = weights[start:end]  # Slice the weight vector for the biases\n",
    "    bs.append(b)  # Append the bias vector to the list\n",
    "\n",
    "    # Unpack bias vectors for the hidden layers\n",
    "    for i in range(NUM_HIDDEN_LAYERS - 1):  # Loop through hidden layers\n",
    "        start = end  # Update the start index for the next bias vector\n",
    "        end = end + NUM_HIDDEN[i+1]  # Calculate the end index for the next bias vector\n",
    "        b = weights[start:end]  # Slice the weight vector for the biases\n",
    "        bs.append(b)  # Append the bias vector to the list\n",
    "\n",
    "    # Unpack the bias vector for the output layer\n",
    "    start = end  # Set start index for the output layer's bias\n",
    "    end = end + NUM_OUTPUT  # Calculate the end index for the output layer's bias vector\n",
    "    b = weights[start:end]  # Slice the weight vector for the biases\n",
    "    bs.append(b)  # Append the bias vector to the list\n",
    "\n",
    "    # Return the unpacked weight matrices and bias vectors\n",
    "    return Ws, bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaa1c6b",
   "metadata": {},
   "source": [
    "### Initializing the weights and biases\n",
    "\n",
    "Initializes the weights and biases for a multi-layer neural network using a modified Kaiming He Uniform initialization for weights and a small constant for biases.\n",
    "    \n",
    "Arguments:\n",
    "- NUM_HIDDEN_LAYERS: Number of hidden layers in the neural network.\n",
    "- NUM_INPUT: Number of input features.\n",
    "- NUM_HIDDEN: List specifying the number of units in each hidden layer.\n",
    "- NUM_OUTPUT: Number of output units.\n",
    "\n",
    "Returns:\n",
    "- Ws: List of initialized weight matrices.\n",
    "- bs: List of initialized bias vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f70889d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initWeightsAndBiases(NUM_HIDDEN_LAYERS, NUM_INPUT, NUM_HIDDEN, NUM_OUTPUT):\n",
    "    \n",
    "    Ws = []  # List to store weight matrices\n",
    "    bs = []  # List to store bias vectors\n",
    "\n",
    "    # Strategy:\n",
    "    # - Weight initialization: Kaiming He Uniform initialization, suitable for ReLU activation functions.\n",
    "    # - Bias initialization: Small positive constant (0.01).\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Initialize weights between input layer and the first hidden layer\n",
    "    W = 2 * (np.random.random(size=(NUM_HIDDEN[0], NUM_INPUT)).astype(np.float32) / NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "    Ws.append(W)  # Add the initialized weight matrix to the list\n",
    "    \n",
    "    # Initialize biases for the first hidden layer\n",
    "    b = 0.01 * np.ones(NUM_HIDDEN[0], dtype=np.float32)  # Bias initialized to a small positive number (0.01)\n",
    "    bs.append(b)  # Add the initialized bias vector to the list\n",
    "\n",
    "    # Initialize weights and biases for hidden layers\n",
    "    for i in range(NUM_HIDDEN_LAYERS - 1):\n",
    "        # Initialize weights between the ith hidden layer and the next hidden layer\n",
    "        W = 2 * (np.random.random(size=(NUM_HIDDEN[i+1], NUM_HIDDEN[i])).astype(np.float32) / NUM_HIDDEN[i]**0.5) - 1./NUM_HIDDEN[i]**0.5\n",
    "        Ws.append(W)  # Add the weight matrix to the list\n",
    "        \n",
    "        # Initialize biases for the next hidden layer\n",
    "        b = 0.01 * np.ones(NUM_HIDDEN[i+1], dtype=np.float32)  # Bias initialized to 0.01\n",
    "        bs.append(b)  # Add the bias vector to the list\n",
    "\n",
    "    # Initialize weights for the final output layer (from last hidden layer to output)\n",
    "    W = 2 * (np.random.random(size=(NUM_OUTPUT, NUM_HIDDEN[-1])).astype(np.float32) / NUM_HIDDEN[-1]**0.5) - 1./NUM_HIDDEN[-1]**0.5\n",
    "    Ws.append(W)  # Add the weight matrix to the list\n",
    "    \n",
    "    # Initialize biases for the output layer\n",
    "    b = 0.01 * np.ones(NUM_OUTPUT, dtype=np.float32)  # Bias initialized to 0.01\n",
    "    bs.append(b)  # Add the bias vector to the list\n",
    "\n",
    "    # Return the lists of weight matrices and bias vectors\n",
    "    return Ws, bs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61ce7926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def relu(z):\n",
    "    \n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "def softmax(Z):\n",
    "    # to avoid large exponent values, we will use the deviation idealogy of subtracting each value with the max value , thereby keeping the order preserved and since it will be normalised , result wont matter\n",
    "    \n",
    "    exponent_z = np.exp(Z - np.max(Z,axis=1,keepdims=True))\n",
    "    \n",
    "    prediction = exponent_z/(np.sum(exponent_z,axis=1,keepdims=True))\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "\n",
    "def fCE (X, Y, weights):\n",
    "    Ws, bs = unpack(weights)\n",
    "    H_activation,_= forward_pass(X,Ws,bs,NUM_HIDDEN_LAYERS)\n",
    "    y_pred = H_activation[-1]\n",
    "    ce = loss(Y,y_pred,Ws,0)\n",
    "    return ce\n",
    "\n",
    "def gradCE(X, Y, weights):\n",
    "    # Unpack the flattened weights and biases\n",
    "    Ws, bs = unpack(weights)\n",
    "    \n",
    "    wt_grad = []  # List to store weight gradients\n",
    "    bi_grad = []  # List to store bias gradients\n",
    "\n",
    "    # Perform a forward pass through the network\n",
    "    H_activation, z_score = forward_pass(X, Ws, bs, NUM_HIDDEN_LAYERS)\n",
    "    \n",
    "    # Initialize one-hot encoded labels (y_onehot)\n",
    "    y_onehot = np.zeros_like(H_activation[-1])\n",
    "    \n",
    "    # Convert Y to integer type if needed\n",
    "    Y = Y.astype(int)\n",
    "    \n",
    "    # One-hot encode the ground truth labels\n",
    "    y_onehot[np.arange(len(Y)), Y] = 1\n",
    "\n",
    "    # Compute the gradient of the loss w.r.t. the output layer (softmax layer)\n",
    "    g = H_activation[-1] - y_onehot  # Difference between prediction and ground truth\n",
    "    \n",
    "    # Backpropagate through each layer, starting from the output layer\n",
    "    for i in range(NUM_HIDDEN_LAYERS, -1, -1):\n",
    "        # Gradient of weights (g^T * H) and biases (sum of gradients over batch size)\n",
    "        grad_wt = np.dot(g.T, H_activation[i]) / len(Y)\n",
    "        grad_bi = np.sum(g, axis=0, keepdims=True) / len(Y)\n",
    "        \n",
    "        # Append the computed gradients to the respective lists\n",
    "        wt_grad.append(grad_wt)\n",
    "        bi_grad.append(grad_bi)\n",
    "        \n",
    "        if i > 0:  # If not the first layer (input layer)\n",
    "            # Backpropagate the gradient to the previous layer using the ReLU derivative\n",
    "            g = np.dot(g, Ws[i]) * relu_derivative(z_score[i-1])\n",
    "    \n",
    "    # Reverse the gradients list so they align correctly with the weight matrices\n",
    "    wt_grad.reverse()\n",
    "    bi_grad.reverse()\n",
    "\n",
    "    # Flatten and concatenate all gradients (weights and biases) into a single vector\n",
    "    allGradientsAsVector = np.hstack([W.flatten() for W in wt_grad] + [b.flatten() for b in bi_grad])\n",
    "    \n",
    "    return allGradientsAsVector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f329418",
   "metadata": {},
   "source": [
    "### Loss Function ###\n",
    "\n",
    "Computes the cross-entropy loss between the true labels (y_label) and the predicted\n",
    "probabilities (y_pred) for a batch of data. Optionally, it also supports L2 regularization.\n",
    "\n",
    "Arguments:\n",
    "- y_label: Ground truth labels (as integer indices).\n",
    "- y_pred: Predicted probabilities from the softmax function.\n",
    "- W: Weight matrix (used for optional L2 regularization, currently not used).\n",
    "- alpha: Regularization parameter (L2 regularization strength, currently not used).\n",
    "\n",
    "Returns:\n",
    "- loss: The average cross-entropy loss for the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc5c9df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_label, y_pred, W, alpha):\n",
    "\n",
    "    # Get the batch size, i.e., number of samples in the current batch.\n",
    "    batch_s = y_label.shape[0]  # This ensures loss is computed as an average across the batch.\n",
    "\n",
    "    # Ensure that y_label is of integer type, as it represents class indices.\n",
    "    y_label = y_label.astype(int)\n",
    "\n",
    "    # Compute the negative log-likelihood (cross-entropy loss) for the true class labels.\n",
    "    # For each sample, we access the predicted probability for the true label.\n",
    "    prob = -np.log(y_pred[range(batch_s), y_label])  # Select the predicted probability for the true label using indexing.\n",
    "    \n",
    "    # Compute the total loss by summing all the individual losses, and then averaging over the batch.\n",
    "    loss = np.sum(prob) / batch_s  # Average loss across the batch.\n",
    "    \n",
    "    # Optionally, L2 regularization (currently commented out).\n",
    "    # It penalizes large weights to prevent overfitting, and helps to smooth the loss landscape.\n",
    "    # reg_loss = alpha / 2 * np.sum(np.square(W))  # L2 regularization term (currently not used).\n",
    "    \n",
    "    # Compute total batch loss (currently not using regularization).\n",
    "    # batch_loss = loss + reg_loss  # Total loss (including regularization).\n",
    "    \n",
    "    return loss  # Return the average cross-entropy loss (without regularization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f813f",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "Performs the forward pass through a neural network, computing activations for each layer.\n",
    "\n",
    "Arguments:\n",
    "- x: Input data (batch of samples).\n",
    "- Weights: List of weight matrices for each layer.\n",
    "- bias: List of bias vectors for each layer.\n",
    "- NUM_HIDDEN_LAYERS: Number of hidden layers in the network.\n",
    "\n",
    "Returns:\n",
    "- H_activation: A list containing the activations for each layer (including input and output layers).\n",
    "- z_preactivation_score: A list containing the pre-activation scores (z values) for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "247070a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, Weights, bias, NUM_HIDDEN_LAYERS):\n",
    "    \n",
    "    z_preactivation_score = []  # To store the pre-activation (z) values for each layer.\n",
    "    H_activation = [x]  # List to store activations, starting with the input data (H0 = x).\n",
    "    \n",
    "    # Loop over each hidden layer to compute pre-activations (z) and activations (H).\n",
    "    for i in range(NUM_HIDDEN_LAYERS):\n",
    "        # Compute the pre-activation score for the i-th layer: z = H_prev * W.T + b\n",
    "        z = np.dot(H_activation[-1], Weights[i].T) + bias[i]\n",
    "        \n",
    "        # Store the pre-activation score (before applying the activation function).\n",
    "        z_preactivation_score.append(z)\n",
    "        \n",
    "        # Apply the ReLU activation function to the pre-activation score to get the next activation.\n",
    "        H = relu(z)\n",
    "        \n",
    "        # Append the activation (H) to the list of activations.\n",
    "        H_activation.append(H)\n",
    "    \n",
    "    # For the output layer (final layer), apply softmax to get predicted probabilities (y_pred).\n",
    "    z = np.dot(H_activation[-1], Weights[-1].T) + bias[-1]  # Pre-activation for the output layer.\n",
    "    \n",
    "    # Store the pre-activation score for the output layer.\n",
    "    z_preactivation_score.append(z)\n",
    "    \n",
    "    # Apply softmax to the final pre-activation score to compute the predicted probabilities (output).\n",
    "    y_pred = softmax(z)\n",
    "    \n",
    "    # Append the output layer activation (y_pred) to the list of activations.\n",
    "    H_activation.append(y_pred)\n",
    "    \n",
    "    # Return both the activations and the pre-activation scores for each layer.\n",
    "    return H_activation, z_preactivation_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cdf302",
   "metadata": {},
   "source": [
    "### Back Propogation\n",
    "\n",
    "Performs the backpropagation algorithm to compute gradients and update weights and biases\n",
    "using gradient descent with L2 regularization.\n",
    "\n",
    "Arguments:\n",
    "- x_batch: Batch of input data.\n",
    "- y_batch: Batch of true labels (ground truth).\n",
    "- h_activations: List of activations from the forward pass (including input and output).\n",
    "- z_score: List of pre-activation scores (z values) from the forward pass.\n",
    "- Weights: List of weight matrices for each layer.\n",
    "- bias: List of bias vectors for each layer.\n",
    "- lr: Learning rate for gradient descent.\n",
    "- alpha: L2 regularization strength.\n",
    "- NUM_HIDDEN_LAYERS: Number of hidden layers in the neural network.\n",
    "\n",
    "Returns:\n",
    "- Weights: Updated weight matrices after gradient descent.\n",
    "- bias: Updated bias vectors after gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01ba8d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(x_batch, y_batch, h_activations, z_score, Weights, bias, lr, alpha, NUM_HIDDEN_LAYERS):\n",
    "    \n",
    "    wt_gradient = []  # To store gradients for weight matrices\n",
    "    bias_gradient = []  # To store gradients for bias vectors\n",
    "    batch_s = y_batch.shape[0]  # Get the batch size\n",
    "    \n",
    "    # Create a one-hot encoded matrix for the ground truth labels\n",
    "    y_onehot = np.zeros_like(h_activations[-1])  # Shape is same as the output layer (y_pred)\n",
    "    y_onehot[np.arange(batch_s), y_batch] = 1  # Set the correct label index for each sample in the batch\n",
    "\n",
    "    # Compute the initial gradient of the loss with respect to the output (y_pred)\n",
    "    g = h_activations[-1] - y_onehot  # Gradient of cross-entropy loss w.r.t. output (softmax layer)\n",
    "\n",
    "    # Loop over each layer, starting from the output layer and going backward\n",
    "    for i in range(NUM_HIDDEN_LAYERS, -1, -1):\n",
    "        # Compute the gradient of the weight matrix for the current layer\n",
    "        grad_wt = np.dot(g.T, h_activations[i]) / batch_s  # Divide by batch size for averaging\n",
    "        grad_bi = np.sum(g, axis=0, keepdims=True) / batch_s  # Compute gradient of the bias vector\n",
    "        \n",
    "        # Store the gradients for later use\n",
    "        wt_gradient.append(grad_wt)\n",
    "        bias_gradient.append(grad_bi)\n",
    "        \n",
    "        # Propagate the gradient to the previous layer (if not the input layer)\n",
    "        if i > 0:\n",
    "            g = np.dot(g, Weights[i]) * relu_derivative(z_score[i - 1])  # Chain rule application\n",
    "            \n",
    "    # Reverse the gradients list to maintain the correct order (from first layer to last)\n",
    "    wt_gradient.reverse()\n",
    "    bias_gradient.reverse()\n",
    "\n",
    "    # Gradient descent update step for weights and biases\n",
    "    for i in range(NUM_HIDDEN_LAYERS + 1):\n",
    "        # Update weights using gradient descent and L2 regularization (alpha * Weights[i])\n",
    "        Weights[i] -= lr * (wt_gradient[i] + alpha * Weights[i])\n",
    "        # Update bias using gradient descent\n",
    "        bias[i] -= lr * bias_gradient[i].reshape(-1)\n",
    "    \n",
    "    return Weights, bias  # Return the updated weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9257cf",
   "metadata": {},
   "source": [
    "### Train Function\n",
    "\n",
    "Train the neural network using stochastic gradient descent (SGD) with mini-batches.\n",
    "\n",
    "Arguments:\n",
    "- X_train: Training data inputs (features).\n",
    "- y_train: Training data labels (targets).\n",
    "- weights: Initial flattened weights and biases (single vector).\n",
    "- X_val: Validation data inputs (optional for validation purposes).\n",
    "- y_val: Validation data labels (optional for validation purposes).\n",
    "- lr: Learning rate for gradient descent.\n",
    "- num_epochs: Number of training epochs.\n",
    "- batch_size: Size of mini-batches for SGD.\n",
    "- alpha: L2 regularization strength.\n",
    "- NUM_HIDDEN_LAYERS: Number of hidden layers in the neural network.\n",
    "- NUM_INPUT: Number of input features (size of input layer).\n",
    "- NUM_HIDDEN: List of sizes for each hidden layer.\n",
    "- NUM_OUTPUT: Number of output classes (size of output layer).\n",
    "\n",
    "Returns:\n",
    "- WTS: Flattened final weights and biases after training.\n",
    "- avg_loss: Average loss over all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ff4cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, weights, X_val, y_val, lr, num_epochs, batch_size, alpha, NUM_HIDDEN_LAYERS, NUM_INPUT, NUM_HIDDEN, NUM_OUTPUT):\n",
    "    \n",
    "    num_tr_samples = X_train.shape[0]  # Total number of training samples\n",
    "    tr_indices = np.arange(num_tr_samples)  # Create an array of sample indices for shuffling\n",
    "\n",
    "    # Unpack the initial weights and biases from the flattened vector\n",
    "    Weights, bias = unpack(weights, NUM_HIDDEN_LAYERS, NUM_INPUT, NUM_HIDDEN, NUM_OUTPUT)\n",
    "    \n",
    "    total_loss = 0  # Initialize total loss accumulator\n",
    "\n",
    "    # Training loop for multiple epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        np.random.shuffle(tr_indices)  # Shuffle the training sample indices at the start of each epoch\n",
    "        \n",
    "        # Iterate over mini-batches\n",
    "        for num in range(0, num_tr_samples, batch_size):\n",
    "            # Select a mini-batch of data based on the current indices\n",
    "            batch_index = tr_indices[num: num + batch_size]\n",
    "            x_batch = X_train[batch_index]  # Extract the inputs for the mini-batch\n",
    "            y_batch = y_train[batch_index]  # Extract the labels for the mini-batch\n",
    "            \n",
    "            # Perform a forward pass to compute activations and pre-activations\n",
    "            H_activations, z_list = forward_pass(x_batch, Weights, bias, NUM_HIDDEN_LAYERS)\n",
    "   \n",
    "            # Perform backpropagation and update weights and biases\n",
    "            Weights, bias = backprop(x_batch, y_batch, H_activations, z_list, Weights, bias, lr, alpha, NUM_HIDDEN_LAYERS)\n",
    "        \n",
    "        # After each epoch, calculate the loss for the entire training set\n",
    "        activations, _ = forward_pass(X_train, Weights, bias, NUM_HIDDEN_LAYERS)\n",
    "        epoch_loss = loss(y_train, activations[-1], Weights, alpha)  # Compute the loss after one epoch\n",
    "        \n",
    "        total_loss += epoch_loss  # Accumulate the total loss over epochs\n",
    "\n",
    "    # After training, flatten the final weights and biases into a single vector\n",
    "    WTS = np.hstack([W.flatten() for W in Weights] + [b.flatten() for b in bias])\n",
    "    \n",
    "    # Calculate the average loss over all epochs\n",
    "    avg_loss = total_loss / num_epochs\n",
    "    \n",
    "    return WTS, avg_loss  # Return the final weights and average loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f10197",
   "metadata": {},
   "source": [
    "### Validate Function\n",
    "\n",
    "Validate the performance of the neural network on a validation dataset.\n",
    "\n",
    "Arguments:\n",
    "- X_val: Validation data inputs (features).\n",
    "- y_val: Validation data labels (targets).\n",
    "- weights: Current weights and biases of the model (flattened vector).\n",
    "- NUM_HIDDEN_LAYERS: Number of hidden layers in the neural network.\n",
    "- NUM_INPUT: Number of input features (size of input layer).\n",
    "- NUM_HIDDEN: List of sizes for each hidden layer.\n",
    "- NUM_OUTPUT: Number of output classes (size of output layer).\n",
    "\n",
    "Returns:\n",
    "- accuracy: The accuracy of the model on the validation dataset (fraction of correct predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec36b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(X_val, y_val, weights, NUM_HIDDEN_LAYERS, NUM_INPUT, NUM_HIDDEN, NUM_OUTPUT):\n",
    "    \n",
    "    # Unpack the weights and biases from the flattened weights vector\n",
    "    Weights, bias = unpack(weights, NUM_HIDDEN_LAYERS, NUM_INPUT, NUM_HIDDEN, NUM_OUTPUT)\n",
    "    \n",
    "    # Perform forward pass on the validation data to compute activations\n",
    "    activations, _ = forward_pass(X_val, Weights, bias, NUM_HIDDEN_LAYERS)\n",
    "    \n",
    "    # The last element of activations corresponds to the output layer predictions\n",
    "    y_pred = activations[-1]\n",
    "\n",
    "    # Get the predicted class labels by taking the index of the maximum predicted probability\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Calculate the accuracy by comparing predicted labels with the true labels\n",
    "    accuracy = np.mean(y_pred_labels == y_val)  # Fraction of correct predictions\n",
    "    \n",
    "    # Optionally print the validation accuracy (commented out)\n",
    "    # print(f'Validation accuracy: {accuracy * 100:.2f}%')\n",
    "    \n",
    "    return accuracy  # Return the computed accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523320ff",
   "metadata": {},
   "source": [
    "### Validation function\n",
    "\n",
    "Validate the performance of the neural network with various hyperparameters.\n",
    "\n",
    "Arguments:\n",
    "- X_train: Training data inputs (features).\n",
    "- y_train: Training data labels (targets).\n",
    "- X_val: Validation data inputs.\n",
    "- y_val: Validation data labels.\n",
    "\n",
    "Returns:\n",
    "- best_hyperparameters: Dictionary of the best hyperparameters found.\n",
    "- best_weights: Weights corresponding to the best model.\n",
    "- best_accuracy: Highest accuracy achieved on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae1829fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    # Possible hyperparameter values to test\n",
    "    NUM_HIDDEN_LAYERS = [3, 4, 5]  # Number of hidden layers to try\n",
    "    NUM_HIDDEN_UNITS = [30, 40, 50]  # Number of units in each hidden layer\n",
    "    learning_rates = [0.005, 0.01, 0.05]  # Different learning rates\n",
    "    mini_batch_sizes = [16, 32, 64]  # Mini-batch sizes for training\n",
    "    num_epochs_testing = [50, 100]  # Number of epochs for training\n",
    "    \n",
    "    # Initialize best values for tracking the best model\n",
    "    best_accuracy = 0  # Highest accuracy observed so far\n",
    "    best_hyperparameters = {}  # To store the best hyperparameters\n",
    "    best_weights, best_bias = None, None  # To store the best weights and biases\n",
    "\n",
    "    # Loop through all combinations of hyperparameters\n",
    "    for num_hidden_layer in NUM_HIDDEN_LAYERS:\n",
    "        for num_hidden_units in NUM_HIDDEN_UNITS:\n",
    "            for lr in learning_rates:\n",
    "                for batch in mini_batch_sizes:\n",
    "                    for epoch in num_epochs_testing:\n",
    "                        \n",
    "                        # Define input and output dimensions\n",
    "                        NUM_INPUT = 784  # Number of input features (e.g., for MNIST)\n",
    "                        NUM_OUTPUT = 10  # Number of output classes (e.g., digits 0-9)\n",
    "\n",
    "                        # Create a list representing the number of hidden units in each hidden layer\n",
    "                        NUM_HIDDEN = num_hidden_layer * [num_hidden_units]\n",
    "\n",
    "                        # Initialize weights and biases using the helper function\n",
    "                        Ws, bs = initWeightsAndBiases(num_hidden_layer, NUM_INPUT, NUM_HIDDEN, NUM_OUTPUT)\n",
    "                        \n",
    "                        # Flatten weights and biases into a single array for optimization\n",
    "                        weights = np.hstack([W.flatten() for W in Ws] + [b.flatten() for b in bs])\n",
    "                        weights = weights.astype(np.float32)  # Ensure the data type is float32\n",
    "\n",
    "                        # Train the model and obtain the final weights and loss\n",
    "                        weights, test_loss = train(X_train, y_train, weights, X_val, y_val, lr, epoch, batch, 0, num_hidden_layer, NUM_INPUT, NUM_HIDDEN, NUM_OUTPUT)\n",
    "                        \n",
    "                        # Validate the model and obtain the accuracy\n",
    "                        accuracy = validate(X_val, y_val, weights, num_hidden_layer, NUM_INPUT, NUM_HIDDEN, NUM_OUTPUT)\n",
    "\n",
    "                        # Print the results for the current set of hyperparameters\n",
    "                        print(f\"n_hidden_lrs: {num_hidden_layer}, n_hidden_units: {num_hidden_units}, Num_Epoch {epoch}, Batch_size: {batch}, Learning_rate: {lr}, Unregularise CE: {test_loss:.5f}, Test accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "                        # Check if the current accuracy is the best we've seen\n",
    "                        if accuracy > best_accuracy:\n",
    "                            best_accuracy = accuracy  # Update best accuracy\n",
    "                            best_hyperparameters = {'num_hidden_layers': num_hidden_layer,\n",
    "                                                    'num_hidden_units': num_hidden_units,\n",
    "                                                    'num_epochs': epoch,\n",
    "                                                    'learning_rate': lr,\n",
    "                                                    'mini_batch': batch}  # Update best hyperparameters\n",
    "                            best_weights = weights  # Update best weights\n",
    "\n",
    "    return best_hyperparameters, best_weights, best_accuracy  # Return the best hyperparameters, weights, and accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2469dcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.32630, Test accuracy: 88.42%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.26063, Test accuracy: 88.33%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.39904, Test accuracy: 88.08%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.31940, Test accuracy: 88.31%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.50693, Test accuracy: 87.18%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.40018, Test accuracy: 87.94%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.28510, Test accuracy: 87.86%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.22689, Test accuracy: 88.15%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.32700, Test accuracy: 88.27%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.26238, Test accuracy: 87.74%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.40073, Test accuracy: 87.92%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.32183, Test accuracy: 88.33%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.24958, Test accuracy: 88.12%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.20744, Test accuracy: 87.59%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.25572, Test accuracy: 87.88%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.20749, Test accuracy: 87.67%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.28181, Test accuracy: 87.67%\n",
      "n_hidden_lrs: 3, n_hidden_units: 30, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.22720, Test accuracy: 88.20%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.32104, Test accuracy: 87.61%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.25273, Test accuracy: 87.90%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.39352, Test accuracy: 87.65%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.31330, Test accuracy: 88.41%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.50302, Test accuracy: 86.64%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.39749, Test accuracy: 88.07%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.27818, Test accuracy: 88.17%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.21370, Test accuracy: 87.83%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.32220, Test accuracy: 88.11%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.25348, Test accuracy: 88.25%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.39502, Test accuracy: 87.78%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.31586, Test accuracy: 88.48%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.23836, Test accuracy: 87.74%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.19074, Test accuracy: 87.72%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.24766, Test accuracy: 88.13%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.18703, Test accuracy: 87.92%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.27433, Test accuracy: 88.28%\n",
      "n_hidden_lrs: 3, n_hidden_units: 40, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.20983, Test accuracy: 87.71%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.30996, Test accuracy: 88.13%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.23381, Test accuracy: 88.26%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.38461, Test accuracy: 87.87%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.30075, Test accuracy: 88.25%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.48955, Test accuracy: 86.96%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.38732, Test accuracy: 87.83%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.26127, Test accuracy: 88.87%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.19300, Test accuracy: 87.72%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.30968, Test accuracy: 88.31%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.23356, Test accuracy: 87.72%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.38642, Test accuracy: 87.88%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.30322, Test accuracy: 88.44%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.21518, Test accuracy: 88.88%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.16139, Test accuracy: 88.49%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.22928, Test accuracy: 88.94%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.16554, Test accuracy: 88.14%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.25549, Test accuracy: 88.91%\n",
      "n_hidden_lrs: 3, n_hidden_units: 50, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.18844, Test accuracy: 87.84%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.34213, Test accuracy: 87.58%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.26851, Test accuracy: 88.22%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.43789, Test accuracy: 87.44%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.33528, Test accuracy: 87.92%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.57906, Test accuracy: 86.29%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.43385, Test accuracy: 87.70%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.29218, Test accuracy: 88.02%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.23121, Test accuracy: 87.65%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.34461, Test accuracy: 87.95%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.27162, Test accuracy: 87.61%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.44075, Test accuracy: 87.13%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.33854, Test accuracy: 87.89%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.26414, Test accuracy: 88.08%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.21927, Test accuracy: 88.38%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.26909, Test accuracy: 86.41%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.21624, Test accuracy: 88.33%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.29056, Test accuracy: 87.91%\n",
      "n_hidden_lrs: 4, n_hidden_units: 30, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.23219, Test accuracy: 87.50%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.32807, Test accuracy: 87.61%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.25158, Test accuracy: 87.79%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.42190, Test accuracy: 87.46%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.31919, Test accuracy: 88.03%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.56127, Test accuracy: 86.55%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.41796, Test accuracy: 87.98%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.27402, Test accuracy: 87.54%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.20889, Test accuracy: 88.20%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.32969, Test accuracy: 87.34%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.25179, Test accuracy: 87.69%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.42338, Test accuracy: 87.35%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.32006, Test accuracy: 88.05%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.23637, Test accuracy: 88.52%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.18638, Test accuracy: 88.33%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.24867, Test accuracy: 88.10%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.19205, Test accuracy: 87.71%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.27560, Test accuracy: 88.28%\n",
      "n_hidden_lrs: 4, n_hidden_units: 40, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.21109, Test accuracy: 87.92%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.31297, Test accuracy: 88.34%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.23247, Test accuracy: 88.12%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.40999, Test accuracy: 87.06%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.30409, Test accuracy: 88.02%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.54707, Test accuracy: 87.09%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.40833, Test accuracy: 87.91%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.26310, Test accuracy: 88.06%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.19244, Test accuracy: 87.77%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.31467, Test accuracy: 88.42%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.23423, Test accuracy: 88.28%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.41082, Test accuracy: 87.98%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.30566, Test accuracy: 87.86%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.22106, Test accuracy: 88.77%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.16626, Test accuracy: 88.21%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.23255, Test accuracy: 88.88%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.17376, Test accuracy: 88.02%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.25681, Test accuracy: 88.77%\n",
      "n_hidden_lrs: 4, n_hidden_units: 50, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.19070, Test accuracy: 88.28%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.36462, Test accuracy: 87.95%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.27802, Test accuracy: 87.65%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.46978, Test accuracy: 87.42%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.34839, Test accuracy: 86.95%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.64601, Test accuracy: 86.72%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.46421, Test accuracy: 86.72%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.29463, Test accuracy: 87.96%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.23269, Test accuracy: 87.81%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.36498, Test accuracy: 87.85%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.27918, Test accuracy: 87.52%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.46861, Test accuracy: 87.67%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.34862, Test accuracy: 86.98%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.26709, Test accuracy: 88.19%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.22066, Test accuracy: 87.61%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.27815, Test accuracy: 87.92%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.22603, Test accuracy: 87.87%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.29409, Test accuracy: 87.69%\n",
      "n_hidden_lrs: 5, n_hidden_units: 30, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.23522, Test accuracy: 87.31%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.34704, Test accuracy: 87.88%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.26026, Test accuracy: 87.02%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.45056, Test accuracy: 86.88%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.33143, Test accuracy: 87.65%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.61760, Test accuracy: 86.42%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.44523, Test accuracy: 87.75%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.28520, Test accuracy: 88.10%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.21934, Test accuracy: 87.71%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.35156, Test accuracy: 87.86%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.26351, Test accuracy: 87.76%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.45621, Test accuracy: 87.19%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.33745, Test accuracy: 88.00%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.24585, Test accuracy: 88.62%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.19403, Test accuracy: 88.08%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.25801, Test accuracy: 88.52%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.20017, Test accuracy: 88.33%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.28437, Test accuracy: 87.55%\n",
      "n_hidden_lrs: 5, n_hidden_units: 40, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.21927, Test accuracy: 88.31%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.34737, Test accuracy: 87.46%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.005, Unregularise CE: 0.25020, Test accuracy: 88.33%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.45078, Test accuracy: 87.53%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.005, Unregularise CE: 0.32638, Test accuracy: 87.28%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.62869, Test accuracy: 86.67%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.005, Unregularise CE: 0.44709, Test accuracy: 87.12%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.27505, Test accuracy: 88.34%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.01, Unregularise CE: 0.20062, Test accuracy: 88.12%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.34837, Test accuracy: 88.22%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.01, Unregularise CE: 0.25007, Test accuracy: 88.05%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.45412, Test accuracy: 87.52%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.01, Unregularise CE: 0.32729, Test accuracy: 87.88%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 50, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.23020, Test accuracy: 87.77%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 100, Batch_size: 16, Learning_rate: 0.05, Unregularise CE: 0.17500, Test accuracy: 87.95%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 50, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.24024, Test accuracy: 88.53%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 100, Batch_size: 32, Learning_rate: 0.05, Unregularise CE: 0.17558, Test accuracy: 87.77%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 50, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.26747, Test accuracy: 88.11%\n",
      "n_hidden_lrs: 5, n_hidden_units: 50, Num_Epoch 100, Batch_size: 64, Learning_rate: 0.05, Unregularise CE: 0.19745, Test accuracy: 86.88%\n"
     ]
    }
   ],
   "source": [
    "training_data = np.reshape(np.load(\"fashion_mnist_train_images.npy\"), (-1, 28*28))\n",
    "training_labels = np.load(\"fashion_mnist_train_labels.npy\")\n",
    "\n",
    "num_datapoints = training_data.shape[0]\n",
    "split_index = int(0.8*num_datapoints)\n",
    "indices = np.arange(num_datapoints)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:split_index]\n",
    "val_indices = indices[split_index:]\n",
    "\n",
    "X_train, X_val = training_data[train_indices], training_data[val_indices]   \n",
    "y_train, y_val = training_labels[train_indices], training_labels[val_indices]\n",
    "\n",
    "X_train = X_train / 255.0 - 0.5\n",
    "X_val = X_val / 255.0 - 0.5\n",
    "\n",
    "best_hyp,best_weights,best_acc= validation(X_train,y_train,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00b7951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"best_model_weights_bp.npy\", best_weights)\n",
    "np.save(\"best_model_hyperparameters_bp.npy\",best_hyp)\n",
    "np.save(\"best_model_accuracy_bp.npy\",best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87371c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.94166666666666"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38607053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
